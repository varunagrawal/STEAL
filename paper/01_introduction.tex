\section{Introduction}

The goal of roboticists is to enable general-purpose robots that can work with human collaborators to achieve greater efficiency and productivity than the current status quo.

To accomplish the task set for the robot, two criteria must be satisfied: the robot should be able to succeed at the task in new environments outside of the scope for which it was originally programmed, and it should be able to perform the task safely, reliably while allowing for failure analysis.

Learning-based techniques have emerged as a major paradigm to enable robot adaptivity. Learning from demonstrations (LfD)~\cite{Argall09ras} in particular is a powerful approach to teach robots new skills or the ability operate in new environments. This enables the robot to achieve task success with a small number of demonstrations as per a demonstrator-determined metric.
Safety, reliability, and interpretability are important characteristics required to allow for the widespread deployment of robotics. Safety and reliability can help invoke confidence in the general masses in the sustainability of robot solutions, while interpretability can aid in debugging the failure cases that may occur, and allow for putting in appropriate safeguards. These characteristics have been extensively researched in the robotic control literature, and many theoretical and practical guarantees can be made.

To get the best of both worlds, we leverage the recently proposed RMPFlow framework~\cite{Cheng21tase} for generating stable, locally reactive trajectories in non-Euclidean spaces (or manifolds). RMPFlow operates on subtask manifold spaces where each subtask can be connected in a tree-based data structure. Using the calculus of Riemannian Motion Policies (RMPs), the optimal acceleration policies for each subtask can be combined to provide a globally optimal and stable policy, satisfying the second criteria detailed above. Alternatively, we can learn the subtask map and Riemannian metric for the RMP corresponding to the desired task~\cite{Rana20corl,Rana20ldc}, which RMPFlow can then combine optimally, thus achieving the first criteria. The structure imposed by the RMP also makes learning from a limited number of demonstrations viable, thus making it a natural fit for LfD techniques.

The current drawback of existing works leveraging learning with RMPflow is that they rely on expert demonstrations. The teacher in each of these cases is assumed to be aware of the underlying robot structure and capabilities. This severely limits the widespread adoption of robotics due to the complexity and expense involved in training the task force. To address this shortcoming, we would like to accommodate non-expert, suboptimal trajectories in an intuitive way.

Gaussian Processes~\cite{Rasmussen04book} are a powerful mechanism for probabilistically representing trajectories~\cite{vanWaveren22manuscript}. A Gaussian Process (GP) is a non-parametric model which represents a distribution over functions, where any subset of these function values is jointly gaussian. Due to their non-parametric nature, GPs are fully specified by the Gaussian Process prior and the data provided. GPs are also highly interpretable since their covariance functions (a.k.a. kernel) provide structure and encode knowledge about the data distribution.

In this work, we propose to leverage Gaussian Processes with an appropriate kernel definition to accommodate suboptimal demonstration trajectories. By computing the posterior of the GP, we can learn the optimal mean trajectory for the task while being flexible to newly provided trajectories.
To train the RMP for use in RMPflow, we can sample trajectories from the posterior GP and use that as our set of expert demonstrations. The posterior GP can also provide us with highly variable trajectories, which have been shown to improve policy learning~\cite{Duan17neurips}.

\Varun{Add a paragraph about results}

% %===============================================================================
% \vspace{-1em}
% \section{Description}

% To generate a stable, reactive robot policy, we leverage RMPFlow~\cite{Rana20corl}. RMPFlow provides a mathematically sound framework for generating guaranteed Lyapunov stable and reactive policies by leveraging Riemannian Geometry. This is related to prior work in motion policy generation such as DMP~\cite{Schaal06amam} but generalize over methods such as DMP and Model Predictive Control~\cite{Ratliff18arxiv}, making them a state-of-the-art framework for policy generation.

% We propose to leverage the framework described by~\cite{Rana20corl}, to learn a controller from potentially suboptimal demonstrations. This is accomplised by learning the Riemannian Motion Policy (RMP) potential functions $\Phi$ and the corresponding Riemannian metric \textbf{M}.
% %This is similar to learning the potential functions over classical impedance control~\cite{KhansariZadeh17ar} but with the added benefits of the Riemannian Geometry, providing us with reactive, safe, stable and hopefully optimal controllers.

% A key drawback of~\cite{Rana20corl} is that all the trajectories provided to the algorithm are given by a human expert. Thus the demonstration suboptimality is controlled to a certain extent as the selected nominal trajectory is from the dataset of expert trajectories and will be close to the other trajectories in the task space, something which may not be true in many real-world applications.

% Instead of picking one trajectory from the dataset as the nominal trajectory (via the use of Dynamic Time Warping), we propose to model the nominal trajectories throuhgh the use of a Gaussian Process. A gaussian process (GP) is a mathematical framework for probabilistically modeling continuous time functions. Here each function is the trajectory $\zeta(t)$ indexed by time $t$. GPs have seen significant application in the robotics community as a way to represent continuous-time trajectories~\cite{Barfoot14rss,Dong18icra} and thus form a natural choice for our proposed method. Additionally, GPs possess desirable properties such as exact sparsity and a low data requirement. Since GPs are estimated via a posterior probability, we can further expand their utilities and constraints by adding information via the priors (e.g. Mutual Information).

% As a stretch goal, a question we would like to answer as well is whether this framework can also handle heterogenous demonstrations. Given $N$ different datasets, each for a specific skill, it would be interesting to understand the pros and cons of our approach in such a heterogenous regime.

% %===============================================================================
% \section{Data}

% \subsection{Simulation}

% To establish correctness of our algorithm, we wish to first execute our proposed framework on a simple object push task in simulation. The goal of this task is to teach a manipulator to push a block over a line from few, teleoperated demonstrations. We can add further constraints such as performing the same task with clutter.

% For a real world task, we propose to use Assistive Gym's Robot Feeding Task as a dataset.
% Assistive Gym~\cite{Erickson20icra} provides a simulated environment for Robot Healthcare integrated into OpenAI's Gym environment.
% Feeding is a non-trivial task due to various considerations such as the human in the loop as well as voluntary and involuntary motion from the human to whom assistance is being provided. We believe this task can demonstrate the benefits of our approach very well.

% \subsection{Human Data}

% As a stretch goal, if time permits, we would like to collect data from human demonstrations as well for a simple pick and place task. However, we wish to first see success in the simulation cases.

% %===============================================================================

% \section{Data Collection Protocol}

% \subsection{Simulation}

% For both sets of simulated tasks, data will be collected via teleoperation. To encourage suboptimality, each of our team members will provide demonstrations without seeing each other's provided demonstrations. If need be, we will recruite friends for additional variance in the dataset.

% \subsection{Human Data}

% Human demonstrations would be collected either via Teleoperation or Kinesthetic Teaching of a Franka Arm or a Sawyer robot since they are robots available to us at Georgia Tech.

% %===============================================================================

% \section{Expected Outcome}

% The expected outcome of our approach is that we are able to learn a policy generated from RMPFlow which achieves the proposed tasks successfully and is adaptable to both dynamic environment changes as well as new information provided (via the Gaussian Process prior or the RMP constraints)., given suboptimal demonstrations.

% Our hope is that the use of RMPFlow implies lower risk to humans during deployment, but a potential risk is the gap between our provided demonstrations and the expectation of the end-users.

% %===============================================================================

% \section{Identification of Benchmark}

% We will compare our framework to the one proposed in~\cite{Rana20corl}. A simple metric we can use is ratio of task success as well as time for task execution.

% %===============================================================================

% \section{Timeline}
% \begin{itemize}
%     \item Week 1 Setup all the necessary software.
%     \item Week 2 Data collection for push task and Assistive Gym task.
%     \item Week 3 Project design and implementation.
%     \item Week 4-6 Evaluation and debugging.
%     \item Week 7 Report and presentation creation.
% \end{itemize}
